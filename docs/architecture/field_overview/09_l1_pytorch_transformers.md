PAGE 9 – Layer 1: PyTorch & Transformers

Title: L1 – Models & Code: PyTorch & Transformers

What this layer does (continued)

PyTorch & Autograd

Let us build field-aware models:

Cross-encoder rerankers tuned to Gibsey’s characters/symbols.

Classifiers (e.g. “field tension” or “gift/hoard” dynamics).

Gradients let us optimize for:

Grounding + polyphony + correct citations, not just generic loss.

Transformers

Architecture of any LLM we call (GPT, Gemini, Claude).

Architecture of any custom smaller models we might train (e.g. an Arieol-style reranker).

Question answered: What kind of mind is running on top of the metal?